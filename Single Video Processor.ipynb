{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9eea67c-4d0d-418a-85ff-6e3aee6558df",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72097003-a9f5-4fe5-bfb7-e95c29ddaab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-08-05 00:23:54.286490: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to hybrid_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load and initialize the model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Define the path to save the model\n",
    "model_path = 'hybrid_model.pth'\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a181599-3594-496c-b832-4c6c56dc9778",
   "metadata": {},
   "source": [
    "# Process Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732f2b64-48cd-4d41-883c-9a37447a016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a28fcd8-fd06-4544-97c9-bafa3f2c2ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0, Confidence: 0.09\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Constants\n",
    "N_FRAMES = 3\n",
    "HEIGHT = 112\n",
    "WIDTH = 112\n",
    "MAX_TEXT_FEATURES = 3\n",
    "\n",
    "# Define the path to the saved model\n",
    "model_path = './models/hybrid_model.pth'\n",
    "\n",
    "# Initialize the model architecture\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Load the saved state dictionary into the model\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_video(video_path, n_frames):\n",
    "    \"\"\"\n",
    "    Preprocess video by extracting frames and resizing them to the required dimensions.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        n_frames (int): Number of frames to extract.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of processed video frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < n_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (WIDTH, HEIGHT))\n",
    "        frame = frame / 255.0  # Normalize pixel values to [0, 1]\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    # If not enough frames, pad with zeros\n",
    "    if len(frames) < n_frames:\n",
    "        frames.extend([np.zeros((HEIGHT, WIDTH, 3))] * (n_frames - len(frames)))\n",
    "\n",
    "    return np.array(frames)\n",
    "\n",
    "def analyze_video(video_path, model, text_features):\n",
    "    \"\"\"\n",
    "    Analyze a video and return the predicted class and confidence.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        model (torch.nn.Module): Pretrained model for classification.\n",
    "        text_features (str): Additional text features.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted class.\n",
    "        float: Confidence score of the prediction.\n",
    "    \"\"\"\n",
    "    # Preprocess video\n",
    "    video_frames = preprocess_video(video_path, N_FRAMES)\n",
    "    video_frames = np.expand_dims(video_frames, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Convert video frames to PyTorch tensor\n",
    "    video_frames_tensor = torch.tensor(video_frames, dtype=torch.float32).permute(0, 4, 1, 2, 3).to(device)  # (N, C, T, H, W)\n",
    "\n",
    "    # Tokenize text features\n",
    "    encoded_text = tokenizer(text_features, padding='max_length', truncation=True, max_length=MAX_TEXT_FEATURES, return_tensors='pt')\n",
    "    input_ids = encoded_text['input_ids'].to(device)\n",
    "    attention_mask = encoded_text['attention_mask'].to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.cpu().numpy()\n",
    "\n",
    "    predicted_class = np.argmax(logits, axis=-1)[0]\n",
    "    confidence = logits[0][predicted_class]\n",
    "\n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Example usage\n",
    "video_path = 'test.mp4'\n",
    "text_features = \"Example text feature\"  # Example text input\n",
    "\n",
    "predicted_class, confidence = analyze_video(video_path, model, text_features)\n",
    "print(f\"Predicted class: {predicted_class}, Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b707fc3-1bad-4bfe-907c-6060c367576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0, Confidence: 0.09\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Constants\n",
    "N_FRAMES = 3\n",
    "HEIGHT = 112\n",
    "WIDTH = 112\n",
    "MAX_TEXT_FEATURES = 3\n",
    "\n",
    "# Define the path to the saved model\n",
    "model_path = './models/bert_model.pth'\n",
    "\n",
    "# Initialize the model architecture\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Load the saved state dictionary into the model\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_video(video_path, n_frames):\n",
    "    \"\"\"\n",
    "    Preprocess video by extracting frames and resizing them to the required dimensions.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        n_frames (int): Number of frames to extract.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of processed video frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < n_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (WIDTH, HEIGHT))\n",
    "        frame = frame / 255.0  # Normalize pixel values to [0, 1]\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    # If not enough frames, pad with zeros\n",
    "    if len(frames) < n_frames:\n",
    "        frames.extend([np.zeros((HEIGHT, WIDTH, 3))] * (n_frames - len(frames)))\n",
    "\n",
    "    return np.array(frames)\n",
    "\n",
    "def analyze_video(video_path, model, text_features):\n",
    "    \"\"\"\n",
    "    Analyze a video and return the predicted class and confidence.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        model (torch.nn.Module): Pretrained model for classification.\n",
    "        text_features (str): Additional text features.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted class.\n",
    "        float: Confidence score of the prediction.\n",
    "    \"\"\"\n",
    "    # Preprocess video\n",
    "    video_frames = preprocess_video(video_path, N_FRAMES)\n",
    "    video_frames = np.expand_dims(video_frames, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Convert video frames to PyTorch tensor\n",
    "    video_frames_tensor = torch.tensor(video_frames, dtype=torch.float32).permute(0, 4, 1, 2, 3).to(device)  # (N, C, T, H, W)\n",
    "\n",
    "    # Tokenize text features\n",
    "    encoded_text = tokenizer(text_features, padding='max_length', truncation=True, max_length=MAX_TEXT_FEATURES, return_tensors='pt')\n",
    "    input_ids = encoded_text['input_ids'].to(device)\n",
    "    attention_mask = encoded_text['attention_mask'].to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.cpu().numpy()\n",
    "\n",
    "    predicted_class = np.argmax(logits, axis=-1)[0]\n",
    "    confidence = logits[0][predicted_class]\n",
    "\n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Example usage\n",
    "video_path = 'test.mp4'\n",
    "text_features = \"Example text feature\"  # Example text input\n",
    "\n",
    "predicted_class, confidence = analyze_video(video_path, model, text_features)\n",
    "print(f\"Predicted class: {predicted_class}, Confidence: {confidence:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
